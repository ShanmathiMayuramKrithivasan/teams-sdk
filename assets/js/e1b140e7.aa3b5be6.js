"use strict";(self.webpackChunkteams_md=self.webpackChunkteams_md||[]).push([[914],{26802:(e,n,t)=>{t.d(n,{A:()=>a});var s=t(22147),i=t(62540);function a({language:e,children:n}){return(0,s.zy)().pathname.includes(`/${e}/`)?(0,i.jsx)(i.Fragment,{children:n}):null}},27023:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/streaming-chat-d49edc84e71626c46703881e19aef8c2.gif"},43023:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(63696);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},87989:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>l,frontMatter:()=>r,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"python/in-depth-guides/ai/chat","title":"\ud83d\udcac Chat Generation","description":"\x3c!--","source":"@site/docs/main/python/in-depth-guides/ai/chat.mdx","sourceDirName":"python/in-depth-guides/ai","slug":"/python/in-depth-guides/ai/chat","permalink":"/teams-sdk/python/in-depth-guides/ai/chat","draft":false,"unlisted":false,"editUrl":"https://github.com/microsoft/teams-sdk/tree/main/teams.md/docs/main/python/in-depth-guides/ai/chat.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"\ud83d\udcac Chat Generation","title":"\ud83d\udcac Chat Generation","summary":"Comprehensive guide to implementing chat generation with LLMs in Teams, covering setup with ChatPrompt and Model objects, basic message handling, and streaming responses for improved user experience."},"sidebar":"default","previous":{"title":"Setup & Prerequisites","permalink":"/teams-sdk/python/in-depth-guides/ai/setup-and-prereqs"},"next":{"title":"Function Calling","permalink":"/teams-sdk/python/in-depth-guides/ai/function-calling"}}');var i=t(62540),a=t(43023),o=t(26802);const r={sidebar_position:2,sidebar_label:"\ud83d\udcac Chat Generation",title:"\ud83d\udcac Chat Generation",summary:"Comprehensive guide to implementing chat generation with LLMs in Teams, covering setup with ChatPrompt and Model objects, basic message handling, and streaming responses for improved user experience."},d="\ud83d\udcac Chat Generation",c={},h=[{value:"Simple chat generation",id:"simple-chat-generation",level:2},{value:"Agent",id:"agent",level:3},{value:"Streaming chat responses",id:"streaming-chat-responses",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",mermaid:"mermaid",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"-chat-generation",children:"\ud83d\udcac Chat Generation"})}),"\n",(0,i.jsxs)(n.p,{children:["Before going through this guide, please make sure you have completed the ",(0,i.jsx)(n.a,{href:"/teams-sdk/python/in-depth-guides/ai/setup-and-prereqs",children:"setup and prerequisites"})," guide."]}),"\n",(0,i.jsx)(n.h1,{id:"setup",children:"Setup"}),"\n",(0,i.jsxs)(n.p,{children:["The basic setup involves creating a ",(0,i.jsx)(n.code,{children:"ChatPrompt"})," and giving it the ",(0,i.jsx)(n.code,{children:"Model"})," you want to use."]}),"\n",(0,i.jsx)(n.mermaid,{value:'flowchart LR\n    Prompt\n\n    subgraph Application\n        Send --\x3e Prompt\n        UserMessage["User Message<br/>Hi how are you?"] --\x3e Send\n        Send --\x3e Content["Content<br/>I am doing great! How can I help you?"]\n\n        subgraph Setup\n            Messages --\x3e Prompt\n            Instructions --\x3e Prompt\n            Options["Other options..."] --\x3e Prompt\n\n            Prompt --\x3e Model\n        end\n    end\n\n    subgraph LLMProvider\n        Model --\x3e AOAI["Azure Open AI"]\n        Model --\x3e OAI["Open AI"]\n        Model --\x3e Anthropic["Claude"]\n        Model --\x3e OtherModels["..."]\n    end'}),"\n",(0,i.jsx)(n.h2,{id:"simple-chat-generation",children:"Simple chat generation"}),"\n",(0,i.jsx)(n.p,{children:"Chat generation is the the most basic way of interacting with an LLM model. It involves setting up your ChatPrompt, the Model, and sending it the message."}),"\n",(0,i.jsxs)(o.A,{language:"python",children:[(0,i.jsx)(n.p,{children:"Import the relevant objects:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from microsoft_teams.ai import ChatPrompt\nfrom microsoft_teams.api import MessageActivity, MessageActivityInput\nfrom microsoft_teams.apps import ActivityContext\nfrom microsoft_teams.openai import OpenAICompletionsAIModel\n"})})]}),"\n",(0,i.jsx)(o.A,{language:"python",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"@app.on_message\nasync def handle_message(ctx: ActivityContext[MessageActivity]):\n    openai_model = OpenAICompletionsAIModel(model=AZURE_OPENAI_MODEL)\n    agent = ChatPrompt(model=openai_model)\n\n    chat_result = await agent.send(\n        input=ctx.activity.text,\n        instructions=\"You are a friendly assistant who talks like a pirate.\"\n    )\n    result = chat_result.response\n    if result.content:\n        await ctx.send(MessageActivityInput(text=result.content).add_ai_generated())\n        # Ahoy, matey! \ud83c\udff4\u200d\u2620\ufe0f How be ye doin' this fine day on th' high seas? What can this ol' salty sea dog help ye with? \ud83d\udea2\u2620\ufe0f\n"})})}),"\n",(0,i.jsx)(o.A,{language:"python",children:(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["The current ",(0,i.jsx)(n.code,{children:"OpenAICompletionsAIModel"})," implementation uses Chat Completions API. The Responses API is also available."]})})}),"\n",(0,i.jsxs)(o.A,{language:"python",children:[(0,i.jsx)(n.h3,{id:"agent",children:"Agent"}),(0,i.jsxs)(n.p,{children:["Instead of ",(0,i.jsx)(n.code,{children:"ChatPrompt"}),", you may also use ",(0,i.jsx)(n.code,{children:"Agent"}),". The ",(0,i.jsx)(n.code,{children:"Agent"})," class is a derivation from ",(0,i.jsx)(n.code,{children:"ChatPrompt"})," but it differs in that it's stateful. The ",(0,i.jsx)(n.code,{children:"memory"})," object passed to the ",(0,i.jsx)(n.code,{children:"Agent"})," object will be reused for subsequent calls to ",(0,i.jsx)(n.code,{children:"send"}),", whereas for ",(0,i.jsx)(n.code,{children:"ChatPrompt"}),", each call to ",(0,i.jsx)(n.code,{children:"send"})," is independent."]})]}),"\n",(0,i.jsx)(n.h2,{id:"streaming-chat-responses",children:"Streaming chat responses"}),"\n",(0,i.jsx)(n.p,{children:"LLMs can take a while to generate a response, so often streaming the response leads to a better, more responsive user experience."}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"Streaming is only currently supported for single 1:1 chats, and not for groups or channels."})}),"\n",(0,i.jsx)(o.A,{language:"python",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from microsoft_teams.ai import ChatPrompt\nfrom microsoft_teams.api import MessageActivity, MessageActivityInput\nfrom microsoft_teams.apps import ActivityContext\nfrom microsoft_teams.openai import OpenAICompletionsAIModel\n# ...\n\n@app.on_message\nasync def handle_message(ctx: ActivityContext[MessageActivity]):\n    openai_model = OpenAICompletionsAIModel(model=AZURE_OPENAI_MODEL)\n    agent = ChatPrompt(model=openai_model)\n\n    chat_result = await agent.send(\n        input=ctx.activity.text,\n        instructions="You are a friendly assistant who responds in terse language.",\n        on_chunk=lambda chunk: ctx.stream.emit(chunk)\n    )\n    result = chat_result.response\n\n    if ctx.activity.conversation.is_group:\n        # If the conversation is a group chat, we need to send the final response\n        # back to the group chat\n        await ctx.send(MessageActivityInput(text=result.content).add_ai_generated())\n    else:\n        ctx.stream.emit(MessageActivityInput().add_ai_generated())\n'})})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Animated image showing agent response text incrementally appearing in the chat window.",src:t(27023).A+"",width:"2006",height:"804"})})]})}function l(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}}}]);