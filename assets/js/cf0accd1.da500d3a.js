"use strict";(self.webpackChunkteams_md=self.webpackChunkteams_md||[]).push([[7730],{26802:(e,n,t)=>{t.d(n,{A:()=>a});var s=t(22147),i=t(62540);function a({language:e,children:n}){return(0,s.zy)().pathname.includes(`/${e}/`)?(0,i.jsx)(i.Fragment,{children:n}):null}},27023:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/streaming-chat-d49edc84e71626c46703881e19aef8c2.gif"},43023:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(63696);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}},72038:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>p,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"typescript/in-depth-guides/ai/chat","title":"\ud83d\udcac Chat Generation","description":"\x3c!--","source":"@site/docs/main/typescript/in-depth-guides/ai/chat.mdx","sourceDirName":"typescript/in-depth-guides/ai","slug":"/typescript/in-depth-guides/ai/chat","permalink":"/teams-sdk/typescript/in-depth-guides/ai/chat","draft":false,"unlisted":false,"editUrl":"https://github.com/microsoft/teams-sdk/tree/main/teams.md/docs/main/typescript/in-depth-guides/ai/chat.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"\ud83d\udcac Chat Generation","title":"\ud83d\udcac Chat Generation","summary":"Comprehensive guide to implementing chat generation with LLMs in Teams, covering setup with ChatPrompt and Model objects, basic message handling, and streaming responses for improved user experience."},"sidebar":"default","previous":{"title":"Setup & Prerequisites","permalink":"/teams-sdk/typescript/in-depth-guides/ai/setup-and-prereqs"},"next":{"title":"Function Calling","permalink":"/teams-sdk/typescript/in-depth-guides/ai/function-calling"}}');var i=t(62540),a=t(43023),r=t(26802);const o={sidebar_position:2,sidebar_label:"\ud83d\udcac Chat Generation",title:"\ud83d\udcac Chat Generation",summary:"Comprehensive guide to implementing chat generation with LLMs in Teams, covering setup with ChatPrompt and Model objects, basic message handling, and streaming responses for improved user experience."},p="\ud83d\udcac Chat Generation",c={},d=[{value:"Simple chat generation",id:"simple-chat-generation",level:2},{value:"Streaming chat responses",id:"streaming-chat-responses",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",mermaid:"mermaid",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"-chat-generation",children:"\ud83d\udcac Chat Generation"})}),"\n",(0,i.jsxs)(n.p,{children:["Before going through this guide, please make sure you have completed the ",(0,i.jsx)(n.a,{href:"/teams-sdk/typescript/in-depth-guides/ai/setup-and-prereqs",children:"setup and prerequisites"})," guide."]}),"\n",(0,i.jsx)(n.h1,{id:"setup",children:"Setup"}),"\n",(0,i.jsxs)(n.p,{children:["The basic setup involves creating a ",(0,i.jsx)(n.code,{children:"ChatPrompt"})," and giving it the ",(0,i.jsx)(n.code,{children:"Model"})," you want to use."]}),"\n",(0,i.jsx)(n.mermaid,{value:'flowchart LR\n    Prompt\n\n    subgraph Application\n        Send --\x3e Prompt\n        UserMessage["User Message<br/>Hi how are you?"] --\x3e Send\n        Send --\x3e Content["Content<br/>I am doing great! How can I help you?"]\n\n        subgraph Setup\n            Messages --\x3e Prompt\n            Instructions --\x3e Prompt\n            Options["Other options..."] --\x3e Prompt\n\n            Prompt --\x3e Model\n        end\n    end\n\n    subgraph LLMProvider\n        Model --\x3e AOAI["Azure Open AI"]\n        Model --\x3e OAI["Open AI"]\n        Model --\x3e Anthropic["Claude"]\n        Model --\x3e OtherModels["..."]\n    end'}),"\n",(0,i.jsx)(n.h2,{id:"simple-chat-generation",children:"Simple chat generation"}),"\n",(0,i.jsx)(n.p,{children:"Chat generation is the the most basic way of interacting with an LLM model. It involves setting up your ChatPrompt, the Model, and sending it the message."}),"\n",(0,i.jsxs)(r.A,{language:"typescript",children:[(0,i.jsx)(n.p,{children:"Import the relevant objects:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { OpenAIChatModel } from '@microsoft/teams.openai';\n"})})]}),"\n",(0,i.jsx)(r.A,{language:"typescript",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { ChatPrompt } from '@microsoft/teams.ai';\nimport { MessageActivity } from '@microsoft/teams.api';\nimport { App } from '@microsoft/teams.apps';\nimport { OpenAIChatModel } from '@microsoft/teams.openai';\n// ...\n\napp.on('message', async ({ send, activity, next, log }) => {\n  const model = new OpenAIChatModel({\n    apiKey: process.env.AZURE_OPENAI_API_KEY || process.env.OPENAI_API_KEY,\n    endpoint: process.env.AZURE_OPENAI_ENDPOINT,\n    apiVersion: process.env.AZURE_OPENAI_API_VERSION,\n    model: process.env.AZURE_OPENAI_MODEL_DEPLOYMENT_NAME!,\n  });\n\n  const prompt = new ChatPrompt({\n    instructions: 'You are a friendly assistant who talks like a pirate',\n    model,\n  });\n\n  const response = await prompt.send(activity.text);\n  if (response.content) {\n    const activity = new MessageActivity(response.content).addAiGenerated();\n    await send(activity);\n    // Ahoy, matey! \ud83c\udff4\u200d\u2620\ufe0f How be ye doin' this fine day on th' high seas? What can this ol' salty sea dog help ye with? \ud83d\udea2\u2620\ufe0f\n  }\n});\n"})})}),"\n",(0,i.jsx)(r.A,{language:"typescript",children:(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["The current ",(0,i.jsx)(n.code,{children:"OpenAIChatModel"})," implementation uses chat-completions API. The responses API is coming soon."]})})}),"\n",(0,i.jsx)(n.h2,{id:"streaming-chat-responses",children:"Streaming chat responses"}),"\n",(0,i.jsx)(n.p,{children:"LLMs can take a while to generate a response, so often streaming the response leads to a better, more responsive user experience."}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"Streaming is only currently supported for single 1:1 chats, and not for groups or channels."})}),"\n",(0,i.jsx)(r.A,{language:"typescript",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { ChatPrompt } from '@microsoft/teams.ai';\nimport { MessageActivity } from '@microsoft/teams.api';\nimport { App } from '@microsoft/teams.apps';\n// ...\n\napp.on('message', async ({ stream, send, activity, next, log }) => {\n  // const query = activity.text;\n\n  const prompt = new ChatPrompt({\n    instructions: 'You are a friendly assistant who responds in extremely verbose language',\n    model,\n  });\n\n  // Notice that we don't `send` the final response back, but\n  // `stream` the chunks as they come in\n  const response = await prompt.send(query, {\n    onChunk: (chunk) => {\n      stream.emit(chunk);\n    },\n  });\n\n  if (activity.conversation.isGroup) {\n    // If the conversation is a group chat, we need to send the final response\n    // back to the group chat\n    const activity = new MessageActivity(response.content).addAiGenerated();\n    await send(activity);\n  } else {\n    // We wrap the final response with an AI Generated indicator\n    stream.emit(new MessageActivity().addAiGenerated());\n  }\n});\n"})})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Animated image showing agent response text incrementally appearing in the chat window.",src:t(27023).A+"",width:"2006",height:"804"})})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);