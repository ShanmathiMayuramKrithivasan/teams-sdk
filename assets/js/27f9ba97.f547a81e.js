"use strict";(self.webpackChunkteams_md=self.webpackChunkteams_md||[]).push([[1154],{26802:(e,n,t)=>{t.d(n,{A:()=>i});var a=t(22147),o=t(62540);function i({language:e,children:n}){return(0,a.zy)().pathname.includes(`/${e}/`)?(0,o.jsx)(o.Fragment,{children:n}):null}},41271:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>s,metadata:()=>a,toc:()=>h});const a=JSON.parse('{"id":"python/in-depth-guides/ai/function-calling","title":"Function Calling","description":"\x3c!--","source":"@site/docs/main/python/in-depth-guides/ai/function-calling.mdx","sourceDirName":"python/in-depth-guides/ai","slug":"/python/in-depth-guides/ai/function-calling","permalink":"/teams-sdk/python/in-depth-guides/ai/function-calling","draft":false,"unlisted":false,"editUrl":"https://github.com/microsoft/teams-sdk/tree/main/teams.md/docs/main/python/in-depth-guides/ai/function-calling.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Function Calling","title":"Function Calling","summary":"How to implement function calling in AI models, allowing the LLM to execute functions as part of its response generation."},"sidebar":"default","previous":{"title":"\ud83d\udcac Chat Generation","permalink":"/teams-sdk/python/in-depth-guides/ai/chat"},"next":{"title":"Keeping State","permalink":"/teams-sdk/python/in-depth-guides/ai/keeping-state"}}');var o=t(62540),i=t(43023),r=t(26802);const s={sidebar_position:3,sidebar_label:"Function Calling",title:"Function Calling",summary:"How to implement function calling in AI models, allowing the LLM to execute functions as part of its response generation."},c="Function / Tool calling",l={},h=[{value:"Multiple functions",id:"multiple-functions",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",mermaid:"mermaid",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"function--tool-calling",children:"Function / Tool calling"})}),"\n",(0,o.jsxs)(n.p,{children:["It's possible to hook up functions that the LLM can decide to call if it thinks it can help with the task at hand. This is done by ",(0,o.jsxs)(r.A,{language:"python",children:["adding a ",(0,o.jsx)(n.code,{children:"function"})," to the ",(0,o.jsx)(n.code,{children:"ChatPrompt"})]}),"."]}),"\n",(0,o.jsx)(r.A,{language:"python",children:(0,o.jsx)(n.mermaid,{value:"sequenceDiagram\n  participant User\n  participant ChatPrompt\n  participant LLM\n  participant Function-PokemonSearch\n  participant ExternalAPI\n\n  User->>ChatPrompt: send(activity.text)\n  ChatPrompt->>LLM: Provide instructions, message, and available functions\n    LLM->>ChatPrompt: Decide to call `pokemon_search` with pokemon_name\n    ChatPrompt->>Function-PokemonSearch: Execute with pokemon_name\n    Function-PokemonSearch->>ExternalAPI: fetch pokemon data\n    ExternalAPI--\x3e>Function-PokemonSearch: return pokemon info\n    Function-PokemonSearch--\x3e>ChatPrompt: return result\n  ChatPrompt->>LLM: Send function result(s)\n  LLM--\x3e>ChatPrompt: Final user-facing response\n  ChatPrompt--\x3e>User: send(result.content)"})}),"\n",(0,o.jsx)(r.A,{language:"python",children:(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import aiohttp\nimport random\nfrom microsoft_teams.ai import Agent, Function\nfrom microsoft_teams.api import MessageActivity, MessageActivityInput\nfrom microsoft_teams.apps import ActivityContext\nfrom microsoft_teams.openai import OpenAICompletionsAIModel\nfrom pydantic import BaseModel\n\nclass SearchPokemonParams(BaseModel):\n    pokemon_name: str\n    """The name of the pokemon."""\n\nasync def pokemon_search_handler(params: SearchPokemonParams) -> str:\n    """Search for Pokemon using PokeAPI - matches documentation example"""\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f"https://pokeapi.co/api/v2/pokemon/{params.pokemon_name.lower()}") as response:\n                if response.status != 200:\n                    raise ValueError(f"Pokemon \'{params.pokemon_name}\' not found")\n\n                data = await response.json()\n\n                result_data = {\n                    "name": data["name"],\n                    "height": data["height"],\n                    "weight": data["weight"],\n                    "types": [type_info["type"]["name"] for type_info in data["types"]],\n                }\n\n                return f"Pokemon {result_data[\'name\']}: height={result_data[\'height\']}, weight={result_data[\'weight\']}, types={\', \'.join(result_data[\'types\'])}"\n    except Exception as e:\n        raise ValueError(f"Error searching for Pokemon: {str(e)}")\n\n@app.on_message\nasync def handle_message(ctx: ActivityContext[MessageActivity]):\n    openai_model = OpenAICompletionsAIModel(model=AZURE_OPENAI_MODEL)\n    agent = Agent(model=openai_model)\n    agent.with_function(\n        Function(\n            name="pokemon_search",\n            description="Search for pokemon information including height, weight, and types",\n            # Include the schema of the parameters\n            # the LLM needs to return to call the function\n            parameter_schema=SearchPokemonParams,\n            handler=pokemon_search_handler,\n        )\n    )\n\n    chat_result = await agent.send(\n            input=ctx.activity.text,\n            instructions="You are a helpful assistant that can look up Pokemon for the user.",\n        )\n\n    if chat_result.response.content:\n        message = MessageActivityInput(text=chat_result.response.content).add_ai_generated()\n        await ctx.send(message)\n    else:\n        await ctx.reply("Sorry I could not find that pokemon")\n'})})}),"\n",(0,o.jsxs)(r.A,{language:"python",children:[(0,o.jsx)(n.h2,{id:"multiple-functions",children:"Multiple functions"}),(0,o.jsxs)(n.p,{children:["Additionally, for complex scenarios, you can add multiple functions to the ",(0,o.jsx)(n.code,{children:"ChatPrompt"}),". The LLM will then decide which function to call based on the context of the conversation. The LLM can pick one or more functions to call before returning the final response."]}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import random\nfrom microsoft_teams.ai import Agent, Function\nfrom microsoft_teams.api import MessageActivity, MessageActivityInput\nfrom microsoft_teams.apps import ActivityContext\nfrom pydantic import BaseModel\n# ...\n\nclass GetLocationParams(BaseModel):\n    """No parameters needed for location"""\n    pass\n\nclass GetWeatherParams(BaseModel):\n    location: str\n    """The location to get weather for"""\n\ndef get_location_handler(params: GetLocationParams) -> str:\n    """Get user location (mock)"""\n    locations = ["Seattle", "San Francisco", "New York"]\n    location = random.choice(locations)\n    return location\n\ndef get_weather_handler(params: GetWeatherParams) -> str:\n    """Get weather for location (mock)"""\n    weather_by_location = {\n        "Seattle": {"temperature": 65, "condition": "sunny"},\n        "San Francisco": {"temperature": 60, "condition": "foggy"},\n        "New York": {"temperature": 75, "condition": "rainy"},\n    }\n\n    weather = weather_by_location.get(params.location)\n    if not weather:\n        return "Sorry, I could not find the weather for that location"\n\n    return f"The weather in {params.location} is {weather[\'condition\']} with a temperature of {weather[\'temperature\']}\xb0F"\n\n@app.on_message\nasync def handle_multiple_functions(ctx: ActivityContext[MessageActivity]):\n    agent = Agent(model)\n\n    agent.with_function(\n        Function(\n            name="get_user_location",\n            description="Gets the location of the user",\n            parameter_schema=GetLocationParams,\n            handler=get_location_handler,\n        )\n    ).with_function(\n        Function(\n            name="weather_search",\n            description="Search for weather at a specific location",\n            parameter_schema=GetWeatherParams,\n            handler=get_weather_handler,\n        )\n    )\n\n    chat_result = await agent.send(\n        input=ctx.activity.text,\n        instructions="You are a helpful assistant that can help the user get the weather. First get their location, then get the weather for that location.",\n    )\n\n    if chat_result.response.content:\n        message = MessageActivityInput(text=chat_result.response.content).add_ai_generated()\n        await ctx.send(message)\n    else:\n        await ctx.reply("Sorry I could not figure it out")\n'})})]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},43023:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var a=t(63696);const o={},i=a.createContext(o);function r(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);