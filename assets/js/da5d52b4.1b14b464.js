"use strict";(self.webpackChunkteams_md=self.webpackChunkteams_md||[]).push([[446],{26802:(e,n,t)=>{t.d(n,{A:()=>i});var s=t(22147),a=t(62540);function i({language:e,children:n}){return(0,s.zy)().pathname.includes(`/${e}/`)?(0,a.jsx)(a.Fragment,{children:n}):null}},27023:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/streaming-chat-d49edc84e71626c46703881e19aef8c2.gif"},43023:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(63696);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}},65965:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>p,default:()=>l,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"csharp/in-depth-guides/ai/chat","title":"\ud83d\udcac Chat Generation","description":"\x3c!--","source":"@site/docs/main/csharp/in-depth-guides/ai/chat.mdx","sourceDirName":"csharp/in-depth-guides/ai","slug":"/csharp/in-depth-guides/ai/chat","permalink":"/teams-sdk/csharp/in-depth-guides/ai/chat","draft":false,"unlisted":false,"editUrl":"https://github.com/microsoft/teams-sdk/tree/main/teams.md/docs/main/csharp/in-depth-guides/ai/chat.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"\ud83d\udcac Chat Generation","title":"\ud83d\udcac Chat Generation","summary":"Comprehensive guide to implementing chat generation with LLMs in Teams, covering setup with ChatPrompt and Model objects, basic message handling, and streaming responses for improved user experience."},"sidebar":"default","previous":{"title":"Setup & Prerequisites","permalink":"/teams-sdk/csharp/in-depth-guides/ai/setup-and-prereqs"},"next":{"title":"Function Calling","permalink":"/teams-sdk/csharp/in-depth-guides/ai/function-calling"}}');var a=t(62540),i=t(43023),r=t(26802);const o={sidebar_position:2,sidebar_label:"\ud83d\udcac Chat Generation",title:"\ud83d\udcac Chat Generation",summary:"Comprehensive guide to implementing chat generation with LLMs in Teams, covering setup with ChatPrompt and Model objects, basic message handling, and streaming responses for improved user experience."},p="\ud83d\udcac Chat Generation",c={},d=[{value:"Simple chat generation",id:"simple-chat-generation",level:2},{value:"Declarative Approach",id:"declarative-approach",level:3},{value:"Streaming chat responses",id:"streaming-chat-responses",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"-chat-generation",children:"\ud83d\udcac Chat Generation"})}),"\n",(0,a.jsxs)(n.p,{children:["Before going through this guide, please make sure you have completed the ",(0,a.jsx)(n.a,{href:"/teams-sdk/csharp/in-depth-guides/ai/setup-and-prereqs",children:"setup and prerequisites"})," guide."]}),"\n",(0,a.jsx)(n.h1,{id:"setup",children:"Setup"}),"\n",(0,a.jsxs)(n.p,{children:["The basic setup involves creating a ",(0,a.jsx)(n.code,{children:"ChatPrompt"})," and giving it the ",(0,a.jsx)(n.code,{children:"Model"})," you want to use."]}),"\n",(0,a.jsx)(n.mermaid,{value:'flowchart LR\n    Prompt\n\n    subgraph Application\n        Send --\x3e Prompt\n        UserMessage["User Message<br/>Hi how are you?"] --\x3e Send\n        Send --\x3e Content["Content<br/>I am doing great! How can I help you?"]\n\n        subgraph Setup\n            Messages --\x3e Prompt\n            Instructions --\x3e Prompt\n            Options["Other options..."] --\x3e Prompt\n\n            Prompt --\x3e Model\n        end\n    end\n\n    subgraph LLMProvider\n        Model --\x3e AOAI["Azure Open AI"]\n        Model --\x3e OAI["Open AI"]\n        Model --\x3e Anthropic["Claude"]\n        Model --\x3e OtherModels["..."]\n    end'}),"\n",(0,a.jsx)(n.h2,{id:"simple-chat-generation",children:"Simple chat generation"}),"\n",(0,a.jsx)(n.p,{children:"Chat generation is the the most basic way of interacting with an LLM model. It involves setting up your ChatPrompt, the Model, and sending it the message."}),"\n",(0,a.jsxs)(r.A,{language:"csharp",children:[(0,a.jsx)(n.p,{children:"Import the relevant namespaces:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:"// AI\nusing Microsoft.Teams.AI.Models.OpenAI;\nusing Microsoft.Teams.AI.Prompts;\n// Teams\nusing Microsoft.Teams.Api.Activities;\nusing Microsoft.Teams.Apps;\nusing Microsoft.Teams.Apps.Activities;\nusing Microsoft.Teams.Apps.Annotations;\n"})}),(0,a.jsx)(n.p,{children:"Create a ChatModel, ChatPrompt, and handle user - LLM interactions:"})]}),"\n",(0,a.jsx)(r.A,{language:"csharp",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using Microsoft.Teams.AI.Models.OpenAI;\nusing Microsoft.Teams.AI.Prompts;\nusing Microsoft.Teams.AI.Templates;\nusing Microsoft.Teams.Api.Activities;\nusing Microsoft.Teams.Apps.Activities;\nusing Azure.AI.OpenAI;\nusing System.ClientModel;\n\n// Configuration\nvar azureOpenAIModel = configuration["AzureOpenAIModel"]!;\nvar azureOpenAIEndpoint = configuration["AzureOpenAIEndpoint"]!;\nvar azureOpenAIKey = configuration["AzureOpenAIKey"]!;\n\nvar azureOpenAI = new AzureOpenAIClient(\n    new Uri(azureOpenAIEndpoint),\n    new ApiKeyCredential(azureOpenAIKey)\n);\n\n// AI Model\nvar aiModel = new OpenAIChatModel(azureOpenAIModel, azureOpenAI);\n\n// Simple chat handler\nteamsApp.OnMessage(async (context) =>\n{\n    var prompt = new OpenAIChatPrompt(aiModel, new ChatPromptOptions\n    {\n        Instructions = new StringTemplate("You are a friendly assistant who talks like a pirate")\n    });\n\n    var result = await prompt.Send(context.Activity.Text);\n    if (result.Content != null)\n    {\n        var messageActivity = new MessageActivity\n        {\n            Text = result.Content,\n        }.AddAIGenerated();\n        await context.Send(messageActivity);\n        // Ahoy, matey! \ud83c\udff4\u200d\u2620\ufe0f How be ye doin\' this fine day on th\' high seas? What can this ol\' salty sea dog help ye with? \ud83d\udea2\u2620\ufe0f\n    }\n});\n'})})}),"\n",(0,a.jsxs)(r.A,{language:"csharp",children:[(0,a.jsx)(n.h3,{id:"declarative-approach",children:"Declarative Approach"}),(0,a.jsx)(n.p,{children:"This approach uses attributes to declare prompts, providing clean separation of concerns."}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Create a Prompt Class:"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'using Microsoft.Teams.AI.Annotations;\n\nnamespace Samples.AI.Prompts;\n\n[Prompt]\n[Prompt.Description("A friendly pirate assistant")]\n[Prompt.Instructions("You are a friendly assistant who talks like a pirate")]\npublic class PiratePrompt\n{\n}\n'})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Usage in Program.cs:"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:"using Microsoft.Teams.AI.Models.OpenAI;\nusing Microsoft.Teams.Api.Activities;\n\n// Create the AI model\nvar aiModel = new OpenAIChatModel(azureOpenAIModel, azureOpenAI);\n\n// Use the prompt with OpenAIChatPrompt.From()\nteamsApp.OnMessage(async (context) =>\n{\n    var prompt = OpenAIChatPrompt.From(aiModel, new Samples.AI.Prompts.PiratePrompt());\n\n    var result = await prompt.Send(context.Activity.Text);\n\n    if (!string.IsNullOrEmpty(result.Content))\n    {\n        await context.Send(new MessageActivity { Text = result.Content }.AddAIGenerated());\n        // Ahoy, matey! \ud83c\udff4\u200d\u2620\ufe0f How be ye doin' this fine day on th' high seas?\n    }\n});\n"})})]}),"\n",(0,a.jsx)(r.A,{language:"csharp",children:(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:["The current ",(0,a.jsx)(n.code,{children:"OpenAIChatModel"})," implementation uses chat-completions API. The responses API is coming soon."]})})}),"\n",(0,a.jsx)(n.h2,{id:"streaming-chat-responses",children:"Streaming chat responses"}),"\n",(0,a.jsx)(n.p,{children:"LLMs can take a while to generate a response, so often streaming the response leads to a better, more responsive user experience."}),"\n",(0,a.jsx)(n.admonition,{type:"warning",children:(0,a.jsx)(n.p,{children:"Streaming is only currently supported for single 1:1 chats, and not for groups or channels."})}),"\n",(0,a.jsx)(r.A,{language:"csharp",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// Streaming handler\nteamsApp.OnMessage(async (context) =>\n{\n    var match = Regex.Match(context.Activity.Text ?? "", @"^stream\\s+(.+)", RegexOptions.IgnoreCase);\n    if (match.Success)\n    {\n        var query = match.Groups[1].Value.Trim();\n        var prompt = new OpenAIChatPrompt(aiModel, new ChatPromptOptions\n        {\n            Instructions = new StringTemplate("You are a friendly assistant who responds in extremely verbose language")\n        });\n\n        var result = await prompt.Send(query, (chunk) =>\n        {\n            context.Stream.Emit(chunk);\n            return Task.CompletedTask;\n        });\n    }\n});\n'})})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Animated image showing agent response text incrementally appearing in the chat window.",src:t(27023).A+"",width:"2006",height:"804"})})]})}function l(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);