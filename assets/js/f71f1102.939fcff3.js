"use strict";(self.webpackChunkteams_md=self.webpackChunkteams_md||[]).push([[4911],{4519:(n,t,e)=>{e.r(t),e.d(t,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"typescript/in-depth-guides/ai/function-calling","title":"Function Calling","description":"\x3c!--","source":"@site/docs/main/typescript/in-depth-guides/ai/function-calling.mdx","sourceDirName":"typescript/in-depth-guides/ai","slug":"/typescript/in-depth-guides/ai/function-calling","permalink":"/teams-sdk/typescript/in-depth-guides/ai/function-calling","draft":false,"unlisted":false,"editUrl":"https://github.com/microsoft/teams-sdk/tree/main/teams.md/docs/main/typescript/in-depth-guides/ai/function-calling.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Function Calling","title":"Function Calling","summary":"How to implement function calling in AI models, allowing the LLM to execute functions as part of its response generation."},"sidebar":"default","previous":{"title":"\ud83d\udcac Chat Generation","permalink":"/teams-sdk/typescript/in-depth-guides/ai/chat"},"next":{"title":"Keeping State","permalink":"/teams-sdk/typescript/in-depth-guides/ai/keeping-state"}}');var i=e(62540),a=e(43023),s=e(26802);const r={sidebar_position:3,sidebar_label:"Function Calling",title:"Function Calling",summary:"How to implement function calling in AI models, allowing the LLM to execute functions as part of its response generation."},c="Function / Tool calling",l={},p=[{value:"Multiple functions",id:"multiple-functions",level:2},{value:"Stopping Functions early",id:"stopping-functions-early",level:2}];function u(n){const t={code:"code",h1:"h1",h2:"h2",header:"header",mermaid:"mermaid",p:"p",pre:"pre",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"function--tool-calling",children:"Function / Tool calling"})}),"\n",(0,i.jsxs)(t.p,{children:["It's possible to hook up functions that the LLM can decide to call if it thinks it can help with the task at hand. This is done by ",(0,i.jsxs)(s.A,{language:"typescript",children:["adding a ",(0,i.jsx)(t.code,{children:"function"})," to the ",(0,i.jsx)(t.code,{children:"ChatPrompt"})]}),"."]}),"\n",(0,i.jsx)(s.A,{language:"typescript",children:(0,i.jsx)(t.mermaid,{value:"sequenceDiagram\n  participant User\n  participant ChatPrompt\n  participant LLM\n  participant Function-PokemonSearch\n  participant ExternalAPI\n\n  User->>ChatPrompt: send(activity.text)\n  ChatPrompt->>LLM: Provide instructions, message, and available functions\n    LLM->>ChatPrompt: Decide to call `pokemonSearch` with parameters\n    ChatPrompt->>Function-PokemonSearch: Execute with pokemonName\n    Function-PokemonSearch->>ExternalAPI: fetch pokemon data\n    ExternalAPI--\x3e>Function-PokemonSearch: return pokemon info\n    Function-PokemonSearch--\x3e>ChatPrompt: return result\n  ChatPrompt->>LLM: Send function result(s)\n  LLM--\x3e>ChatPrompt: Final user-facing response\n  ChatPrompt--\x3e>User: send(result.content)"})}),"\n",(0,i.jsx)(s.A,{language:"typescript",children:(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-typescript",children:"import { ChatPrompt, IChatModel } from '@microsoft/teams.ai';\nimport { ActivityLike, IMessageActivity } from '@microsoft/teams.api';\n// ...\n\nconst prompt = new ChatPrompt({\n  instructions: 'You are a helpful assistant that can look up Pokemon for the user.',\n  model,\n})\n  // Include `function` as part of the prompt\n  .function(\n    'pokemonSearch',\n    'search for pokemon',\n    // Include the schema of the parameters\n    // the LLM needs to return to call the function\n    {\n      type: 'object',\n      properties: {\n        pokemonName: {\n          type: 'string',\n          description: 'the name of the pokemon',\n        },\n      },\n      required: ['text'],\n    },\n    // The cooresponding function will be called\n    // automatically if the LLM decides to call this function\n    async ({ pokemonName }: IPokemonSearch) => {\n      log.info('Searching for pokemon', pokemonName);\n      const response = await fetch(`https://pokeapi.co/api/v2/pokemon/${pokemonName}`);\n      if (!response.ok) {\n        throw new Error('Pokemon not found');\n      }\n      const data = await response.json();\n      // The result of the function call is sent back to the LLM\n      return {\n        name: data.name,\n        height: data.height,\n        weight: data.weight,\n        types: data.types.map((type: { type: { name: string } }) => type.type.name),\n      };\n    }\n  );\n\n// The LLM will then produce a final response to be sent back to the user\n// activity.text could have text like 'pikachu'\nconst result = await prompt.send(activity.text);\nawait send(result.content ?? 'Sorry I could not find that pokemon');\n"})})}),"\n",(0,i.jsxs)(s.A,{language:"typescript",children:[(0,i.jsx)(t.h2,{id:"multiple-functions",children:"Multiple functions"}),(0,i.jsxs)(t.p,{children:["Additionally, for complex scenarios, you can add multiple functions to the ",(0,i.jsx)(t.code,{children:"ChatPrompt"}),". The LLM will then decide which function to call based on the context of the conversation. The LLM can pick one or more functions to call before returning the final response."]}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-typescript",children:"import { ChatPrompt, IChatModel } from '@microsoft/teams.ai';\nimport { ActivityLike, IMessageActivity } from '@microsoft/teams.api';\n// ...\n\n// activity.text could be something like \"what's my weather?\"\n// The LLM will need to first figure out the user's location\n// Then pass that in to the weatherSearch\nconst prompt = new ChatPrompt({\n  instructions: 'You are a helpful assistant that can help the user get the weather',\n  model,\n})\n  // Include multiple `function`s as part of the prompt\n  .function(\n    'getUserLocation',\n    'gets the location of the user',\n    // This function doesn't need any parameters,\n    // so we do not need to provide a schema\n    async () => {\n      const locations = ['Seattle', 'San Francisco', 'New York'];\n      const randomIndex = Math.floor(Math.random() * locations.length);\n      const location = locations[randomIndex];\n      log.info('Found user location', location);\n      return location;\n    }\n  )\n  .function(\n    'weatherSearch',\n    'search for weather',\n    {\n      type: 'object',\n      properties: {\n        location: {\n          type: 'string',\n          description: 'the name of the location',\n        },\n      },\n      required: ['location'],\n    },\n    async ({ location }: { location: string }) => {\n      const weatherByLocation: Record<string, {}> = {\n        Seattle: { temperature: 65, condition: 'sunny' },\n        'San Francisco': { temperature: 60, condition: 'foggy' },\n        'New York': { temperature: 75, condition: 'rainy' },\n      };\n\n      const weather = weatherByLocation[location];\n      if (!weather) {\n        return 'Sorry, I could not find the weather for that location';\n      }\n\n      log.info('Found weather', weather);\n      return weather;\n    }\n  );\n\n// The LLM will then produce a final response to be sent back to the user\nconst result = await prompt.send(activity.text);\nawait send(result.content ?? 'Sorry I could not figure it out');\n"})})]}),"\n",(0,i.jsxs)(s.A,{language:"typescript",children:[(0,i.jsx)(t.h2,{id:"stopping-functions-early",children:"Stopping Functions early"}),(0,i.jsxs)(t.p,{children:["You'll notice that after the function responds, ",(0,i.jsx)(t.code,{children:"ChatPrompt"}),' re-sends the response from the function invocation back to the LLM which responds back with the user-facing message. It\'s possible to prevent this "automatic" function calling by passing in a flag']}),(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-typescript",children:"import { ChatPrompt, IChatModel, Message } from '@microsoft/teams.ai';\nimport { ActivityLike, IMessageActivity } from '@microsoft/teams.api';\n// ...\n\nconst result = await prompt.send(activity.text, {\n  autoFunctionCalling: false, // Disable automatic function calling\n});\n// Extract the function call arguments from the result\nconst functionCallArgs = result.function_calls?.[0].arguments;\n\nconst firstCall = result.function_calls?.[0];\nconst fnResult = actualFunction(firstCall.arguments);\nmessages.push({\n  role: 'function',\n  function_id: firstCall.id,\n  content: fnResult,\n});\n\n// Optionally, you can call the chat prompt again after updating the messages with the results\nconst result = await prompt.send('What should we do next?', {\n  messages,\n  autoFunctionCalling: true, // You can enable it here if you want\n});\nconst functionCallArgs = result.function_calls?.[0].arguments; // Extract the function call arguments\nawait send(\n  `The LLM responed with the following structured output: ${JSON.stringify(functionCallArgs, undefined, 2)}.`\n);\n"})})]})]})}function h(n={}){const{wrapper:t}={...(0,a.R)(),...n.components};return t?(0,i.jsx)(t,{...n,children:(0,i.jsx)(u,{...n})}):u(n)}},26802:(n,t,e)=>{e.d(t,{A:()=>a});var o=e(22147),i=e(62540);function a({language:n,children:t}){return(0,o.zy)().pathname.includes(`/${n}/`)?(0,i.jsx)(i.Fragment,{children:t}):null}},43023:(n,t,e)=>{e.d(t,{R:()=>s,x:()=>r});var o=e(63696);const i={},a=o.createContext(i);function s(n){const t=o.useContext(a);return o.useMemo((function(){return"function"==typeof n?n(t):{...t,...n}}),[t,n])}function r(n){let t;return t=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(a.Provider,{value:t},n.children)}}}]);